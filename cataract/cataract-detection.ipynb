{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Augmentor in c:\\users\\medha\\appdata\\roaming\\python\\python311\\site-packages (0.2.12)\n",
      "Requirement already satisfied: Pillow>=5.2.0 in c:\\users\\medha\\appdata\\roaming\\python\\python311\\site-packages (from Augmentor) (10.3.0)\n",
      "Requirement already satisfied: tqdm>=4.9.0 in c:\\users\\medha\\appdata\\roaming\\python\\python311\\site-packages (from Augmentor) (4.66.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\medha\\appdata\\roaming\\python\\python311\\site-packages (from Augmentor) (1.26.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\medha\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.9.0->Augmentor) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Augmentor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def image_processing(raw_data,data_path,height,width):\n",
    "    class_labels=[]\n",
    "    category_count=0\n",
    "    for i in os.walk(raw_data):\n",
    "        if len(i[2])>0:\n",
    "            counter=0\n",
    "            images=i[2]\n",
    "            class_name=i[0].strip('\\\\')\n",
    "            print(class_name)\n",
    "            path=os.path.join(data_path,class_labels[category_count])\n",
    "            for image in images:\n",
    "                im=cv2.imread(class_name+'\\\\'+image)\n",
    "                im=cv2.resize(im,(height,width))\n",
    "                if not os.path.exists(path):\n",
    "                    os.makedirs(path)\n",
    "                cv2.imwrite(os.path.join(path,str(counter)+'.jpg'),im)\n",
    "                counter+=1\n",
    "            category_count+=1\n",
    "        else:\n",
    "            number_of_classes=len(i[1])\n",
    "            print(number_of_classes,i[1])\n",
    "            class_labels=i[1][:]\n",
    "\n",
    "if __name__=='__main__':\n",
    "    height = 100\n",
    "    width = 100\n",
    "    raw_data = 'rawdata'\n",
    "    data_path = 'data'\n",
    "    if not os.path.exists(data_path):\n",
    "        image_processing(raw_data, data_path, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Parameters\n",
    "raw_data = 'rawdata'\n",
    "data_path = 'data'\n",
    "height = 100\n",
    "width = 100\n",
    "\n",
    "# Check if the data directory exists, if not, create it\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)  # Create the 'data' directory if it doesn't exist\n",
    "    image_processing(raw_data, data_path, height, width)\n",
    "\n",
    "# List all classes in the data directory\n",
    "all_classes = os.listdir(data_path)\n",
    "number_of_classes = len(all_classes)\n",
    "color_channels = 3\n",
    "epochs = 300\n",
    "batch_size = 10\n",
    "batch_counter = 0\n",
    "model_save_name = 'checkpoints/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "#tools for image processing and data handing.\n",
    "class utils:\n",
    "    image_count = []\n",
    "    count_buffer=[]\n",
    "    class_buffer=all_classes[:]\n",
    "    def __init__(self):\n",
    "        self.image_count = []\n",
    "        self.count_buffer = []\n",
    "        for i in os.walk(data_path):\n",
    "            if len(i[2]):\n",
    "                self.image_count.append(len(i[2]))\n",
    "        self.count_buffer=self.image_count[:]\n",
    "\n",
    "    # processing images into arrays and dispatch as batches whenever called.\n",
    "    def batch_dispatch(self,batch_size=batch_size):\n",
    "        global batch_counter\n",
    "        if sum(self.count_buffer):\n",
    "\n",
    "            class_name = random.choice(self.class_buffer)\n",
    "            choice_index = all_classes.index(class_name)\n",
    "            choice_count = self.count_buffer[choice_index]\n",
    "            if choice_count==0:\n",
    "                class_name=all_classes[self.count_buffer.index(max(self.count_buffer))]\n",
    "                choice_index = all_classes.index(class_name)\n",
    "                choice_count = self.count_buffer[choice_index]\n",
    "\n",
    "            slicer=batch_size if batch_size<choice_count else choice_count\n",
    "            img_ind=self.image_count[choice_index]-choice_count\n",
    "            indices=[img_ind,img_ind+slicer]\n",
    "            images = self.generate_images(class_name,indices)\n",
    "            labels = self.generate_labels(class_name,slicer)\n",
    "\n",
    "            self.count_buffer[choice_index]=self.count_buffer[choice_index]-slicer\n",
    "        else:\n",
    "            images,labels=(None,)*2\n",
    "        return images, labels\n",
    "\n",
    "    #gives one hot for the respective labels\n",
    "    def generate_labels(self,class_name,number_of_samples):\n",
    "        one_hot_labels=[0]*number_of_classes\n",
    "        one_hot_labels[all_classes.index(class_name)]=1\n",
    "        one_hot_labels=[one_hot_labels]*number_of_samples\n",
    "        #one_hot_labels=tf.one_hot(indices=[all_classes.index(class_name)]*number_of_samples,depth=number_of_classes)\n",
    "        return one_hot_labels\n",
    "\n",
    "    # image operations\n",
    "    def generate_images(self,class_name,indices):\n",
    "        batch_images=[]\n",
    "        choice_folder=os.path.join(data_path,class_name)\n",
    "        selected_images=os.listdir(choice_folder)[indices[0]:indices[1]]\n",
    "        for image in selected_images:\n",
    "            img=cv2.imread(os.path.join(choice_folder,image))\n",
    "            batch_images.append(img)\n",
    "        return batch_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# augment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "The source directory you specified does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mAugmentor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m folder_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfolder\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m p\u001b[38;5;241m=\u001b[39m \u001b[43mAugmentor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfolder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43msave_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m p\u001b[38;5;241m.\u001b[39mflip_left_right(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m      7\u001b[0m p\u001b[38;5;241m.\u001b[39mblack_and_white(\u001b[38;5;241m0.1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\Augmentor\\Pipeline.py:87\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[1;34m(self, source_directory, output_directory, save_format)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_ground_truth_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_directory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_populate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m                   \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mground_truth_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mground_truth_output_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\Augmentor\\Pipeline.py:135\u001b[0m, in \u001b[0;36mPipeline._populate\u001b[1;34m(self, source_directory, output_directory, ground_truth_directory, ground_truth_output_directory)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Check if the source directory for the original images to augment exists at all\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(source_directory):\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe source directory you specified does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# If a ground truth directory is being specified we will check here if the path exists at all.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ground_truth_directory:\n",
      "\u001b[1;31mOSError\u001b[0m: The source directory you specified does not exist."
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# import Augmentor\n",
    "\n",
    "# folder_name='folder'\n",
    "# p= Augmentor.Pipeline(source_directory=folder_name,save_format=\"png\")\n",
    "# p.flip_left_right(0.5)\n",
    "# p.black_and_white(0.1)\n",
    "# p.gaussian_distortion(probability=0.4, grid_width=7, grid_height=6\n",
    "#                       , magnitude=6, corner=\"ul\", method=\"in\", mex=0.5, mey=0.5, sdx=0.05, sdy=0.05)\n",
    "\n",
    "# p.rotate(0.3, 10,10)\n",
    "# p.skew(0.4,0.5)\n",
    "# p.skew_tilt(0.6,0.8)\n",
    "# p.skew_left_right(0.5, magnitude=0.8)\n",
    "# p.sample(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainer_in_single_file.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8886230746159798999\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 218\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    217\u001b[0m     model \u001b[38;5;241m=\u001b[39m ModelTools()\n\u001b[1;32m--> 218\u001b[0m     network \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m     number_of_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(files) \u001b[38;5;28;01mfor\u001b[39;00m r, d, files \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[0;32m    220\u001b[0m     trainer(network, number_of_images)\n",
      "Cell \u001b[1;32mIn[63], line 149\u001b[0m, in \u001b[0;36mgenerate_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m    146\u001b[0m model \u001b[38;5;241m=\u001b[39m ModelTools()\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# MODEL ARCHITECTURE:\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# level 1 convolution\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m network \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_ph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m network \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpooling_layer(network, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    151\u001b[0m network \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mactivation_layer(network)\n",
      "Cell \u001b[1;32mIn[63], line 55\u001b[0m, in \u001b[0;36mModelTools.conv_layer\u001b[1;34m(self, layer, kernel, input_shape, output_shape, stride_size)\u001b[0m\n\u001b[0;32m     53\u001b[0m stride \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m, stride_size, stride_size, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# does a convolution scan on the given image\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m layer \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSAME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m biases\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m layer\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:91\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[1;34m(self, dtype, name)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Parameters\n",
    "raw_data = 'rawdata'\n",
    "data_path = 'data'\n",
    "height = 100\n",
    "width = 100\n",
    "if not os.path.exists(data_path):\n",
    "    image_processing(raw_data, data_path, height, width)\n",
    "all_classes = os.listdir(data_path)\n",
    "number_of_classes = len(all_classes)\n",
    "color_channels = 3\n",
    "epochs = 300\n",
    "batch_size = 10\n",
    "batch_counter = 0\n",
    "model_save_name = 'checkpoints\\\\'\n",
    "\n",
    "\n",
    "# Create Placeholders for images and labels\n",
    "images_ph = tf.keras.layers.Input(shape=[None, height, width, color_channels], dtype=tf.float32, name='images_ph')\n",
    "labels_ph = tf.keras.layers.Input(shape=[None, number_of_classes], dtype=tf.float32, name='labels_ph')\n",
    "\n",
    "\n",
    "# Model's unit definitions\n",
    "class ModelTools:\n",
    "    # Defined functions for all the basic TensorFlow components that we needed for building a model.\n",
    "    # function definitions are in the respective comments\n",
    "    def add_weights(self, shape):\n",
    "    # a common method to create all sorts of weight connections\n",
    "    # takes in shapes of previous and new layer as a list e.g. [2,10]\n",
    "    # starts with random values of that shape.\n",
    "        return tf.Variable(tf.random.truncated_normal(shape=shape, stddev=0.05))\n",
    "\n",
    "    def add_biases(self, shape):\n",
    "        # a common method to add create biases with default=0.05\n",
    "        # takes in shape of the current layer e.g. x=10\n",
    "        return tf.Variable(tf.constant(0.05, shape=shape))\n",
    "\n",
    "    def conv_layer(self, layer, kernel, input_shape, output_shape, stride_size):\n",
    "        # convolution occurs here.\n",
    "        # create weights and biases for the given layer shape\n",
    "        weights = self.add_weights([kernel, kernel, input_shape, output_shape])\n",
    "        biases = self.add_biases([output_shape])\n",
    "        # stride=[image_jump,row_jump,column_jump,color_jump]=[1,1,1,1] mostly\n",
    "        stride = [1, stride_size, stride_size, 1]\n",
    "        # does a convolution scan on the given image\n",
    "        layer = tf.nn.conv2d(layer, weights, strides=stride, padding='SAME') + biases\n",
    "        return layer\n",
    "\n",
    "    def pooling_layer(self, layer, kernel_size, stride_size):\n",
    "        # basically it reduces the complexity involved by only taking the important features alone\n",
    "        # many types of pooling is there.. average pooling, max pooling..\n",
    "        # max pooling takes the maximum of the given kernel\n",
    "        # kernel=[image_jump,rows,columns,depth]\n",
    "        kernel = [1, kernel_size, kernel_size, 1]\n",
    "        # stride=[image_jump,row_jump,column_jump,color_jump]=[1,2,2,1] mostly\n",
    "        stride = [1, stride_size, stride_size, 1]\n",
    "        return tf.nn.max_pool(layer, ksize=kernel, strides=stride, padding='SAME')\n",
    "\n",
    "    def flattening_layer(self, layer):\n",
    "        # make it single dimensional\n",
    "        input_size = layer.get_shape().as_list()\n",
    "        new_size = input_size[-1] * input_size[-2] * input_size[-3]\n",
    "        return tf.reshape(layer, [-1, new_size]), new_size\n",
    "\n",
    "    def fully_connected_layer(self, layer, input_shape, output_shape):\n",
    "        # create weights and biases for the given layer shape\n",
    "        weights = self.add_weights([input_shape, output_shape])\n",
    "        biases = self.add_biases([output_shape])\n",
    "        # most important operation\n",
    "        layer = tf.matmul(layer, weights) + biases  # mX+b\n",
    "        return layer\n",
    "\n",
    "    def activation_layer(self, layer):\n",
    "        # we use Rectified linear unit Relu. it's the standard activation layer used.\n",
    "        # there are also other layer like sigmoid,tanh..etc. but relu is more efficent.\n",
    "        # function: 0 if x<0 else x.\n",
    "        return tf.nn.relu(layer)\n",
    "\n",
    "\n",
    "# Tools for image processing and data handling.\n",
    "class Utils:\n",
    "    image_count = []\n",
    "    count_buffer = []\n",
    "    class_buffer = all_classes[:]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_count = []\n",
    "        self.count_buffer = []\n",
    "        for i in os.walk(data_path):\n",
    "            if len(i[2]):\n",
    "                self.image_count.append(len(i[2]))\n",
    "        self.count_buffer = self.image_count[:]\n",
    "\n",
    "    # processing images into arrays and dispatch as batches whenever called.\n",
    "    def batch_dispatch(self, batch_size=batch_size):\n",
    "        global batch_counter\n",
    "        if sum(self.count_buffer):\n",
    "            class_name = random.choice(self.class_buffer)\n",
    "            choice_index = all_classes.index(class_name)\n",
    "            choice_count = self.count_buffer[choice_index]\n",
    "            if choice_count == 0:\n",
    "                class_name = all_classes[self.count_buffer.index(max(self.count_buffer))]\n",
    "                choice_index = all_classes.index(class_name)\n",
    "                choice_count = self.count_buffer[choice_index]\n",
    "\n",
    "            slicer = batch_size if batch_size < choice_count else choice_count\n",
    "            img_ind = self.image_count[choice_index] - choice_count\n",
    "            indices = [img_ind, img_ind + slicer]\n",
    "            images = self.generate_images(class_name, indices)\n",
    "            labels = self.generate_labels(class_name, slicer)\n",
    "\n",
    "            self.count_buffer[choice_index] = self.count_buffer[choice_index] - slicer\n",
    "        else:\n",
    "            images, labels = (None, None)\n",
    "        return images, labels\n",
    "\n",
    "    # gives one hot for the respective labels\n",
    "    def generate_labels(self, class_name, number_of_samples):\n",
    "        one_hot_labels = [0] * number_of_classes\n",
    "        one_hot_labels[all_classes.index(class_name)] = 1\n",
    "        one_hot_labels = [one_hot_labels] * number_of_samples\n",
    "        return one_hot_labels\n",
    "\n",
    "    # image operations\n",
    "    def generate_images(self, class_name, indices):\n",
    "        batch_images = []\n",
    "        choice_folder = os.path.join(data_path, class_name)\n",
    "        selected_images = os.listdir(choice_folder)[indices[0]:indices[1]]\n",
    "        for image in selected_images:\n",
    "            img = cv2.imread(os.path.join(choice_folder, image))\n",
    "            batch_images.append(img)\n",
    "        return batch_images\n",
    "\n",
    "\n",
    "# Generating our own model, explanations are given respectively\n",
    "def generate_model():\n",
    "    model = ModelTools()\n",
    "    # MODEL ARCHITECTURE:\n",
    "    # level 1 convolution\n",
    "    network = model.conv_layer(images_ph, 5, 3, 16, 1)\n",
    "    network = model.pooling_layer(network, 5, 2)\n",
    "    network = model.activation_layer(network)\n",
    "    print(network)\n",
    "\n",
    "    # level 2 convolution\n",
    "    network = model.conv_layer(network, 4, 16, 32, 1)\n",
    "    network = model.pooling_layer(network, 4, 2)\n",
    "    network = model.activation_layer(network)\n",
    "    print(network)\n",
    "\n",
    "    # level 3 convolution\n",
    "    network = model.conv_layer(network, 3, 32, 64, 1)\n",
    "    network = model.pooling_layer(network, 3, 2)\n",
    "    network = model.activation_layer(network)\n",
    "    print(network)\n",
    "\n",
    "    # flattening layer\n",
    "    network, features = model.flattening_layer(network)\n",
    "    print(network)\n",
    "\n",
    "    # fully connected layer\n",
    "    network = model.fully_connected_layer(network, features, 1024)\n",
    "    network = model.activation_layer(network)\n",
    "    print(network)\n",
    "\n",
    "    # output layer\n",
    "    network = model.fully_connected_layer(network, 1024, number_of_classes)\n",
    "    print(network)\n",
    "\n",
    "    return network\n",
    "\n",
    "\n",
    "# Training happens here\n",
    "def trainer(network, number_of_images):\n",
    "    tools = Utils()\n",
    "    # find error like squared error but better\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=network, labels=labels_ph)\n",
    "\n",
    "    # now minimize the above error\n",
    "    # calculate the total mean of all the errors from all the nodes\n",
    "    cost = tf.reduce_mean(cross_entropy)\n",
    "    tf.summary.scalar(\"cost\", cost)  # for tensorboard visualization\n",
    "\n",
    "    # Now backpropagate to minimize the cost in the network.\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    session = tf.compat.v1.Session()\n",
    "    session.run(tf.compat.v1.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(model_save_name, graph=tf.get_default_graph())\n",
    "    merged = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver(max_to_keep=4)\n",
    "    counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(int(number_of_images / batch_size)):\n",
    "            counter += 1\n",
    "            images, labels = tools.batch_dispatch()\n",
    "            if images is None:\n",
    "                break\n",
    "            loss, summary = session.run([cost, merged], feed_dict={images_ph: images, labels_ph: labels})\n",
    "            print('loss', loss)\n",
    "            session.run(optimizer, feed_dict={images_ph: images, labels_ph: labels})\n",
    "\n",
    "            print('Epoch number ', epoch, 'batch', batch, 'complete')\n",
    "            writer.add_summary(summary, counter)\n",
    "        saver.save(session, model_save_name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = ModelTools()\n",
    "    network = generate_model()\n",
    "    number_of_images = sum([len(files) for r, d, files in os.walk(\"data\")])\n",
    "    trainer(network, number_of_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#model's unit definitions\n",
    "class model_tools:\n",
    "    # Defined functions for all the basic tensorflow components that we needed for building a model.\n",
    "    # function definitions are in the respective comments\n",
    "\n",
    "    def add_weights(self,shape):\n",
    "        # a common method to create all sorts of weight connections\n",
    "        # takes in shapes of previous and new layer as a list e.g. [2,10]\n",
    "        # starts with random values of that shape.\n",
    "        return tf.Variable(tf.truncated_normal(shape=shape, stddev=0.05))\n",
    "\n",
    "    def add_biases(self,shape):\n",
    "        # a common method to add create biases with default=0.05\n",
    "        # takes in shape of the current layer e.g. x=10\n",
    "        return tf.Variable(tf.constant(0.05, shape=shape))\n",
    "\n",
    "    def conv_layer(self,layer, kernel, input_shape, output_shape, stride_size):\n",
    "        #convolution occurs here.\n",
    "        #create weights and biases for the given layer shape\n",
    "        weights = self.add_weights([kernel, kernel, input_shape, output_shape])\n",
    "        biases = self.add_biases([output_shape])\n",
    "        #stride=[image_jump,row_jump,column_jump,color_jump]=[1,1,1,1] mostly\n",
    "        stride = [1, stride_size, stride_size, 1]\n",
    "        #does a convolution scan on the given image\n",
    "        layer = tf.nn.conv2d(layer, weights, strides=stride, padding='SAME') + biases\n",
    "        return layer\n",
    "\n",
    "    def pooling_layer(self,layer, kernel_size, stride_size):\n",
    "        # basically it reduces the complexity involved by only taking the important features alone\n",
    "        # many types of pooling is there.. average pooling, max pooling..\n",
    "        # max pooling takes the maximum of the given kernel\n",
    "        #kernel=[image_jump,rows,columns,depth]\n",
    "        kernel = [1, kernel_size, kernel_size, 1]\n",
    "        #stride=[image_jump,row_jump,column_jump,color_jump]=[1,2,2,1] mostly\n",
    "        stride = [1, stride_size, stride_size, 1]\n",
    "        return tf.nn.max_pool(layer, ksize=kernel, strides=stride, padding='SAME')\n",
    "\n",
    "    def flattening_layer(self,layer):\n",
    "        #make it single dimensional\n",
    "        input_size = layer.get_shape().as_list()\n",
    "        new_size = input_size[-1] * input_size[-2] * input_size[-3]\n",
    "        return tf.reshape(layer, [-1, new_size]),new_size\n",
    "\n",
    "    def fully_connected_layer(self,layer, input_shape, output_shape):\n",
    "        #create weights and biases for the given layer shape\n",
    "        weights = self.add_weights([input_shape, output_shape])\n",
    "        biases = self.add_biases([output_shape])\n",
    "        #most important operation\n",
    "        layer = tf.matmul(layer,weights) + biases  # mX+b\n",
    "        return layer\n",
    "\n",
    "    def activation_layer(self,layer):\n",
    "        # we use Rectified linear unit Relu. it's the standard activation layer used.\n",
    "        # there are also other layer like sigmoid,tanh..etc. but relu is more efficent.\n",
    "        # function: 0 if x<0 else x.\n",
    "        return tf.nn.relu(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model_architecture.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from build_model import model_tools\n",
    "import tensorflow as tf\n",
    "model=model_tools()\n",
    "\n",
    "def generate_model(images_ph,number_of_classes):\n",
    "    #MODEL ARCHITECTURE:\n",
    "    #level 1 convolution\n",
    "    network=model.conv_layer(images_ph,5,3,16,1)\n",
    "    network=model.pooling_layer(network,5,2)\n",
    "    network=model.activation_layer(network)\n",
    "    print(network)\n",
    "\n",
    "    #level 2 convolution\n",
    "    network=model.conv_layer(network,4,16,32,1)\n",
    "    network=model.pooling_layer(network,4,2)\n",
    "    network=model.activation_layer(network)\n",
    "    print(network)\n",
    "\n",
    "    #level 3 convolution\n",
    "    network=model.conv_layer(network,3,32,64,1)\n",
    "    network=model.pooling_layer(network,3,2)\n",
    "    network=model.activation_layer(network)\n",
    "    print(network)\n",
    "\n",
    "    #flattening layer\n",
    "    network,features=model.flattening_layer(network)\n",
    "    print(network)\n",
    "\n",
    "    #fully connected layer\n",
    "    network=model.fully_connected_layer(network,features,1024)\n",
    "    network=model.activation_layer(network)\n",
    "    print(network)\n",
    "\n",
    "    #output layer\n",
    "    network=model.fully_connected_layer(network,1024,number_of_classes)\n",
    "    print(network)\n",
    "    return network\n",
    "\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    images_ph = tf.placeholder(tf.float32, shape=[None, 100,100,3])\n",
    "    generate_model(images_ph,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "model=model_tools()\n",
    "model_folder='checkpoints'\n",
    "image='49.jpg'\n",
    "img=cv2.imread(image)\n",
    "session=tf.Session()\n",
    "img=cv2.resize(img,(100,100))\n",
    "img=img.reshape(1,100,100,3)\n",
    "labels = np.zeros((1, 2))\n",
    "\n",
    "#Create a saver object to load the model\n",
    "saver = tf.train.import_meta_graph(os.path.join(model_folder,'.meta'))\n",
    "\n",
    "#restore the model from our checkpoints folder\n",
    "saver.restore(session,os.path.join(model_folder,'.\\\\'))\n",
    "\n",
    "#Create graph object for getting the same network architecture\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "#Get the last layer of the network by it's name which includes all the previous layers too\n",
    "network = graph.get_tensor_by_name(\"add_4:0\")\n",
    "\n",
    "#create placeholders to pass the image and get output labels\n",
    "im_ph= graph.get_tensor_by_name(\"Placeholder:0\")\n",
    "label_ph = graph.get_tensor_by_name(\"Placeholder_1:0\")\n",
    "\n",
    "#Inorder to make the output to be either 0 or 1.\n",
    "network=tf.nn.sigmoid(network)\n",
    "\n",
    "# Creating the feed_dict that is required to be fed to calculate y_pred\n",
    "feed_dict_testing = {im_ph: img, label_ph: labels}\n",
    "result=session.run(network, feed_dict=feed_dict_testing)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "session=tf.Session()\n",
    "#create Placeholders for images and labels\n",
    "images_ph=tf.placeholder(tf.float32,shape=[None,height,width,color_channels])\n",
    "labels_ph=tf.placeholder(tf.float32,shape=[None,number_of_classes])\n",
    "\n",
    "#training happens here\n",
    "def trainer(network,number_of_images):\n",
    "    #find error like squared error but better\n",
    "    cross_entropy=tf.nn.softmax_cross_entropy_with_logits_v2(logits=network,labels=labels_ph)\n",
    "\n",
    "    #now minize the above error\n",
    "    #calculate the total mean of all the errors from all the nodes\n",
    "    cost=tf.reduce_mean(cross_entropy)\n",
    "    tf.summary.scalar(\"cost\", cost)#for tensorboard visualisation\n",
    "\n",
    "    #Now backpropagate to minimise the cost in the network.\n",
    "    optimizer=tf.train.AdamOptimizer().minimize(cost)\n",
    "    #print(optimizer)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(model_save_name, graph=tf.get_default_graph())\n",
    "    merged = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver(max_to_keep=4)\n",
    "    counter=0\n",
    "    for epoch in range(epochs):\n",
    "        tools = utils()\n",
    "        for batch in range(int(number_of_images / batch_size)):\n",
    "            counter+=1\n",
    "            images, labels = tools.batch_dispatch()\n",
    "            if images == None:\n",
    "                break\n",
    "            loss,summary = session.run([cost,merged], feed_dict={images_ph: images, labels_ph: labels})\n",
    "            print('loss', loss)\n",
    "            session.run(optimizer, feed_dict={images_ph: images, labels_ph: labels})\n",
    "\n",
    "            print('Epoch number ', epoch, 'batch', batch, 'complete')\n",
    "            writer.add_summary(summary,counter)\n",
    "        saver.save(session, os.path.join(model_save_name))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    tools=utils()\n",
    "    model=model_tools()\n",
    "    network=model_architecture.generate_model(images_ph,number_of_classes)\n",
    "    print (network)\n",
    "    number_of_images = sum([len(files) for r, d, files in os.walk(\"data\")])\n",
    "    trainer(network,number_of_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
